{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a30327-2789-4dbf-9fdf-3ea10d1a43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289d6f0-6a52-4ea5-acbf-dde98812abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"256_512_UNET\"\n",
    "\n",
    "if not os.path.exists(\"./results/{}\".format(experiment_name)):\n",
    "    os.makedirs(\"./results/{}\".format(experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9615ef5-e7b2-4f00-baa4-11dfacc30ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2KDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, lr_transform=None, hr_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.lr_transform = lr_transform\n",
    "        self.hr_transform = hr_transform\n",
    "        self.lr_dir = os.path.join(root_dir, 'LR/X4')\n",
    "        self.hr_dir = os.path.join(root_dir, 'HR')\n",
    "        self.images = [f for f in os.listdir(self.hr_dir) if not f.startswith('.')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_img_name = os.path.join(self.lr_dir, self.images[idx][:-4] + \"x4\" + self.images[idx][-4:])\n",
    "        hr_img_name = os.path.join(self.hr_dir, self.images[idx])\n",
    "        lr_image = Image.open(lr_img_name)\n",
    "        hr_image = Image.open(hr_img_name)\n",
    "        \n",
    "        if self.lr_transform:\n",
    "            lr_image = self.lr_transform(lr_image)\n",
    "        if self.hr_transform:     \n",
    "            hr_image = self.hr_transform(hr_image)\n",
    "\n",
    "        return lr_image, hr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66973704-632f-4ad7-8cb5-a9c0686a3cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_transform = T.Compose([\n",
    "    T.Resize((128,128)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "hr_transform = T.Compose([\n",
    "    T.Resize((512,512)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "root_train = \"./datasets/train/\"\n",
    "root_val = \"./datasets/val/\"\n",
    "\n",
    "train_ds = DIV2KDataset(root_dir=root_train, hr_transform=hr_transform, lr_transform=lr_transform)\n",
    "val_ds = DIV2KDataset(root_dir=root_val, hr_transform=hr_transform, lr_transform=lr_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce2cc5-8c6b-4696-b7f3-c3166863b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers = 4, prefetch_factor = 13, pin_memory_device = 'cuda')\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers = 4, prefetch_factor = 13, pin_memory_device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c80d740-2a53-4909-a82d-3e918a52354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(lr, hr) = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139434bc-8eee-41e2-889e-1b55c974ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_plot = 2\n",
    "fig, axes = plt.subplots(num_samples_to_plot, 2, figsize=(10, 10))\n",
    "\n",
    "samples = [160, 170]\n",
    "for i, sample in enumerate(samples):\n",
    "    lr_image, hr_image = train_ds[sample]\n",
    "    lr_image = np.array(lr_image).transpose(1, 2, 0)  # Transpose LR image data\n",
    "    hr_image = np.array(hr_image).transpose(1, 2, 0)  # Transpose HR image data\n",
    "\n",
    "    axes[i, 0].imshow(lr_image)\n",
    "    axes[i, 0].set_title('LR Image')\n",
    "    axes[i, 1].imshow(hr_image)\n",
    "    axes[i, 1].set_title('HR Image')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2f5d9-ba3c-4d31-a589-13a66fedc482",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed76bec-7dde-460c-81b6-73df410b9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Down-sampling layers\n",
    "        self.down1 = self.conv_block(in_channels, 64)\n",
    "        self.down2 = self.conv_block(64, 128)\n",
    "        self.down3 = self.conv_block(128, 256)\n",
    "        self.down4 = self.conv_block(256, 512)\n",
    "        \n",
    "        # Up-sampling layers\n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up2 = nn.ConvTranspose2d(256*2, 128, kernel_size=2, stride=2)\n",
    "        self.up3 = nn.ConvTranspose2d(128*2, 64, kernel_size=2, stride=2)\n",
    "        self.up4 = nn.ConvTranspose2d(64*2, out_channels, kernel_size=2, stride=2)\n",
    "        self.up5 = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2)\n",
    "        \n",
    "        # # Final conv layer to get the desired number of channels\n",
    "        # self.final = nn.Conv2d(16, out_channels, kernel_size=1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Down-sampling path\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(nn.functional.max_pool2d(down1, 2))\n",
    "        down3 = self.down3(nn.functional.max_pool2d(down2, 2))\n",
    "        down4 = self.down4(nn.functional.max_pool2d(down3, 2))\n",
    "        # print(down4.shape)\n",
    "        # Up-sampling path with skip connections\n",
    "        up1 = self.up1(down4)\n",
    "        up1 = torch.cat([down3, up1], dim=1)\n",
    "        \n",
    "        up2 = self.up2(up1)\n",
    "        up2 = torch.cat([down2, up2], dim=1)\n",
    "        \n",
    "        up3 = self.up3(up2)\n",
    "        up3 = torch.cat([down1, up3], dim=1)\n",
    "        \n",
    "        up4 = self.up4(up3)\n",
    "        up4 = torch.cat([x, up4], dim=1)\n",
    "        up5 = self.up5(up5)\n",
    "        # Final convolutional layer\n",
    "        # out = self.final(up4)\n",
    "        return up5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc69b09-f739-4f77-a6cb-a0554ab04f2c",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c518c9-15b4-48d6-b3df-40090bede917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        vgg19 = models.vgg19(pretrained=True)\n",
    "        self.features = vgg19.features[:35].eval()  # Extract features till conv4_4\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "\n",
    "vgg = VGGFeatureExtractor().cuda()\n",
    "def g_criterion(image1, image2, vgg=vgg):\n",
    "    # Preprocess images\n",
    "    preprocess = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        # T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    image1 = preprocess(image1).cuda()\n",
    "    image2 = preprocess(image2).cuda()\n",
    "\n",
    "    features1 = vgg(image1)\n",
    "    features2 = vgg(image2)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    vgg_loss = criterion(features1, features2)\n",
    "    return vgg_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939cf52b-6b4d-4ecb-a625-4233a7df2ea4",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da3b6e1-e0b3-413c-994d-d72b794812b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "def train(train_dl, model, opt, loss_fn, extra_loss_fn, epochs, save_epochs=[1, 10, 20, 40, 50, 100]):\n",
    "\tloss_plt = torch.Tensor([0 for _ in range(epochs)])\n",
    "\tfor _ in range(epochs):\n",
    "\t\tfor lr, hr in tqdm(train_dl, desc=f'Epoch {_+1}/{epochs}', leave=True):\n",
    "\t\t\tlr, hr = lr.to(device), hr.to(device)\n",
    "\t\t\tsr = model(lr)\n",
    "\t\t\t# print(outputs.grad_fn)\n",
    "\t\t\tloss = loss_fn(hr, sr)\n",
    "\t\t\tloss_plt[_] += loss.item()\n",
    "\t\t\t# print(loss.grad_fn)\n",
    "\t\t\tif opt is not None:\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\topt.step()\n",
    "\t\t\t\topt.zero_grad()\n",
    "\t\tif _+1 in save_epochs:\n",
    "\t\t\tpsnr, ssim = plot_generated_images(model, 3, _+1, 'cuda')\n",
    "\t\t\tfile_path = './results/{}/generator_{}.pth'.format(experiment_name, _+1)\n",
    "\t\t\ttorch.save(model.state_dict(), file_path)\n",
    "\t\t\tprint(\"Epoch [{}/{}]: \\t Running Loss = {} \\t PSNR: {} \\t SSIM: {}\".format(_+1, epochs, loss_plt[_]/len(train_dl), psnr, ssim))\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Epoch [{}/{}]: \\t Running Loss = {}\".format(_+1, epochs, loss_plt[_]/len(train_dl)))\n",
    "\treturn loss_plt, loss_plt/len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fa1e3-6274-4ad5-9b89-ace24b1df776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(G, n_imgs, epoch, device, val_ds=val_ds):\n",
    "    G.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        fig, axs = plt.subplots(n_imgs, 3, figsize=(15, 5 * n_imgs))\n",
    "        psnr = 0\n",
    "        ssim = 0\n",
    "        for i, (lr_img, hr_img) in enumerate(val_ds):\n",
    "            if i >= n_imgs:\n",
    "                break\n",
    "\n",
    "            lr_img = lr_img.to(device).unsqueeze(0)\n",
    "            hr_img = hr_img.to(device).unsqueeze(0)\n",
    "\n",
    "            # Generate super-resolved image\n",
    "            sr_img = G(lr_img)\n",
    "\n",
    "            psnr_value = torchmetrics.functional.image.peak_signal_noise_ratio(sr_img, hr_img).item()\n",
    "            ssim_value = torchmetrics.functional.image.structural_similarity_index_measure(sr_img, hr_img).item() \n",
    "            psnr += psnr_value\n",
    "            ssim += ssim_value\n",
    "            # Move images to CPU for plotting\n",
    "            lr_img = lr_img.cpu().squeeze(0).permute(1, 2, 0)\n",
    "            hr_img = hr_img.cpu().squeeze(0).permute(1, 2, 0)\n",
    "            sr_img = sr_img.cpu().squeeze(0).permute(1, 2, 0)\n",
    "\n",
    "            # Plotting\n",
    "            axs[i, 0].imshow(lr_img)\n",
    "            axs[i, 0].set_title('Low-Resolution')\n",
    "            axs[i, 0].axis('off')\n",
    "            axs[i, 1].imshow(hr_img)\n",
    "            axs[i, 1].set_title('High-Resolution')\n",
    "            axs[i, 1].axis('off')\n",
    "            axs[i, 2].imshow(sr_img)\n",
    "            axs[i, 2].set_title('Super-Resolved')\n",
    "            axs[i, 2].axis('off')\n",
    "\n",
    "            axs[i, 2].text(0.5, -0.1, f'SSIM: {ssim_value:.4f}\\nPSNR: {psnr_value:.2f} dB', horizontalalignment='center', verticalalignment='bottom', transform=axs[i, 2].transAxes, color='black')\n",
    "        psnr /= n_imgs\n",
    "        ssim /= n_imgs\n",
    "\n",
    "    # Save the plot\n",
    "    # axs[-1, 1].text(0.5, -0.1, f'Overall \\nSSIM: {ssim:.4f}\\nPSNR: {psnr:.2f} dB', horizontalalignment='center', verticalalignment='bottom', transform=axs[-1, 1].transAxes color='black')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./results/{experiment_name}/G_{epoch}.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # print(f\"{epoch} \\t PSNR: {psnr:.2f} \\t SSIM:{ssim:.4f}\")\n",
    "\n",
    "    # Set model back to training mode\n",
    "    G.train()\n",
    "    return psnr, ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f85c0-878a-49f9-819d-f0a5c2e2e8ee",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2741f-0e2c-4630-a072-de97f367fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(g, test_dl):\n",
    "    g = g.eval()\n",
    "    device = next(g.parameters()).device\n",
    "    ssim = 0\n",
    "    psnr = 0\n",
    "    for lr, hr in test_dl:\n",
    "        lr = lr.to(device)\n",
    "        hr_img = hr.to(device)\n",
    "\n",
    "        sr_img = g(lr)\n",
    "        psnr += torchmetrics.functional.image.peak_signal_noise_ratio(sr_img, hr_img).item()\n",
    "        ssim += torchmetrics.functional.image.structural_similarity_index_measure(sr_img, hr_img).item()\n",
    "\n",
    "    psnr /= len(test_dl)\n",
    "    ssim /= len(test_dl)\n",
    "    g = g.train()\n",
    "    return psnr, ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3862c-9b7f-4a68-9891-415fdecf07db",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2b888-918b-4066-b9f1-f1e4ab9ff267",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = UNet(3, 3).to('cuda')\n",
    "\n",
    "opt = torch.optim.Adam(g.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62503b4a-b656-44a5-9881-2aab83119de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "\n",
    "print(\"DISCRIMINATOR\")\n",
    "# torchsummary.summary(d, (3, 256, 256))\n",
    "print()\n",
    "print(\"GENERATOR\")\n",
    "torchsummary.summary(g, (3, 128, 128))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44cf2d-d4f7-4065-a5ed-fe64d377ce79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g_loss = train(train_dl, g, opt, criterion, g_criterion, 200, save_epochs = [1, 10, 30, 50, 80, 100, 150, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b335772-892b-4ab7-8313-f54d35cc7279",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr, ssim = evaluate(g, val_dl)\n",
    "\n",
    "print(f\"PSNR: {psnr:.2f} dB, SSIM: {ssim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d99f86-568e-4bff-a8e4-f034bbcc1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(g_loss[-1], label='Generator Loss')\n",
    "# plt.plot(d_loss, label='Discriminator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GAN Training Loss')\n",
    "plt.legend()\n",
    "plt.savefig('losses.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace5ea9-ce99-4e5a-8cef-98c2759bc061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
